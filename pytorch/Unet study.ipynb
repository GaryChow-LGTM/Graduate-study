{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sub-parts of the U-Net model\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# 需要仔细考虑为什么要定义这么多class\n",
    "\n",
    "#实现左边的横向卷积\n",
    "class double_conv(nn.Module):\n",
    "    '''(conv => BN => ReLU) * 2'''\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super(double_conv, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_ch, out_ch, 3, padding=1),\n",
    "            \n",
    "            # 对小批量(mini-batch)3d数据组成的4d输入进行批标准化(Batch Normalization)操作\n",
    "            # 进行批标准化，在训练时，该层计算每次输入的均值与方差，并进行移动平均\n",
    "            nn.BatchNorm2d(out_ch),\n",
    "            \n",
    "            \"\"\"inplace=True\n",
    "            计算结果不会有影响。利用in-place计算可以节省内（显）存，\n",
    "            同时还可以省去反复申请和释放内存的时间。但是会对原变量覆盖，\n",
    "            只要不带来错误就用。\n",
    "            \"\"\"            \n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            # torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True)\n",
    "            # 输入的通道数 / 输出的通道数 / 卷积核大小\n",
    "            nn.Conv2d(out_ch, out_ch, 3, padding=1),\n",
    "            \n",
    "            \n",
    "            # 每次卷积之后做一次批标准化\n",
    "            nn.BatchNorm2d(out_ch),\n",
    "            \n",
    "            # 做完标准化之后再进行ReLU\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "\n",
    "#实现左边第一行的卷积（因为左边第一行开始时不包括池化 所以要先class一个左边第一行，再class包含池化的类\n",
    "class inconv(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super(inconv, self).__init__()\n",
    "        self.conv = double_conv(in_ch, out_ch)  #此处double_conv为上面定义的函数\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "\n",
    "#实现左边的向下池化操作，并完成另一层的卷积\n",
    "class down(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super(down, self).__init__()\n",
    "        self.mpconv = nn.Sequential(\n",
    "            nn.MaxPool2d(2),  # 以池化为2*2的尺寸进行池化\n",
    "            double_conv(in_ch, out_ch)  #池化之后接下一层的卷积\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.mpconv(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class up(nn.Module):\n",
    "    # 对输入数据应用双线性变换\n",
    "    def __init__(self, in_ch, out_ch, bilinear=True):\n",
    "        super(up, self).__init__()\n",
    "        #  would be a nice idea if the upsampling could be learned too,\n",
    "        #  but my machine do not have enough memory to handle all those weights\n",
    "        if bilinear:\n",
    "            self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "            \"\"\"\n",
    "            class torch.nn.Upsample(size=None, scale_factor=None, mode='nearest', align_corners=None)\n",
    "            size:根据不同的输入类型制定的输出大小\n",
    "            scale_factor:指定输出为输入的多少倍数\n",
    "            mode:可使用的上采样算法，有'nearest', 'linear', 'bilinear', 'bicubic' and 'trilinear'. 默认使用'nearest'\n",
    "            align_corners:如果为True，输入的角像素将与输出张量对齐，因此将保存下来这些像素的值。仅当使用的算法为'linear', 'bilinear'or 'trilinear'时可以使用。默认设置为False\n",
    "            \"\"\"\n",
    "        else:\n",
    "            self.up = nn.ConvTranspose2d(in_ch//2, in_ch//2, 2, stride=2)\n",
    "            \"\"\"\n",
    "            如果不为双线性数据，则使用卷积来实现上采样，对于每一条边输入输出的尺寸的公式如下：\n",
    "            output = （input-1）*stride - 2*padding +kernal_size +output_padding\n",
    "            class torch.nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride=1, padding=0, \n",
    "                                                       output_padding=0, groups=1, bias=True, dilation=1)\n",
    "            in_channels(int) – 输入信号的通道数\n",
    "            out_channels(int) – 卷积产生的通道数\n",
    "            kerner_size(int or tuple) - 卷积核的大小\n",
    "            stride(int or tuple,optional) - 卷积步长，即要将输入扩大的倍数。\n",
    "            padding(int or tuple, optional) - 输入的每一条边补充0的层数，高宽都增加2*padding\n",
    "            output_padding(int or tuple, optional) - 输出边补充0的层数，高宽都增加padding\n",
    "            groups(int, optional) – 从输入通道到输出通道的阻塞连接数\n",
    "            bias(bool, optional) - 如果bias=True，添加偏置\n",
    "            dilation(int or tuple, optional) – 卷积核元素之间的间距\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \"\"\"\n",
    "\n",
    "        self.conv = double_conv(in_ch, out_ch)\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        x1 = self.up(x1)\n",
    "        \n",
    "        # input is CHW\n",
    "        diffY = x2.size()[2] - x1.size()[2]\n",
    "        diffX = x2.size()[3] - x1.size()[3]\n",
    "\n",
    "        x1 = F.pad(x1, (diffX // 2, diffX - diffX//2,\n",
    "                        diffY // 2, diffY - diffY//2))\n",
    "        \n",
    "        # for padding issues, see \n",
    "        # https://github.com/HaiyongJiang/U-Net-Pytorch-Unstructured-Buggy/commit/0e854509c2cea854e247a9c615f175f76fbb2e3a\n",
    "        # https://github.com/xiaopeng-liao/Pytorch-UNet/commit/8ebac70e633bac59fc22bb5195e513d5832fb3bd\n",
    "\n",
    "        x = torch.cat([x2, x1], dim=1)\n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class outconv(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super(outconv, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_ch, out_ch, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# full assembly of the sub-parts to form the complete net\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "#from .unet_parts import *\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, n_channels, n_classes):\n",
    "        super(UNet, self).__init__()\n",
    "        self.inc = inconv(n_channels, 64)\n",
    "        self.down1 = down(64, 128)\n",
    "        self.down2 = down(128, 256)\n",
    "        self.down3 = down(256, 512)\n",
    "        self.down4 = down(512, 512)\n",
    "        self.up1 = up(1024, 256)\n",
    "        self.up2 = up(512, 128)\n",
    "        self.up3 = up(256, 64)\n",
    "        self.up4 = up(128, 64)\n",
    "        self.outc = outconv(64, n_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.inc(x)\n",
    "        x2 = self.down1(x1)\n",
    "        x3 = self.down2(x2)\n",
    "        x4 = self.down3(x3)\n",
    "        x5 = self.down4(x4)\n",
    "        x = self.up1(x5, x4)\n",
    "        x = self.up2(x, x3)\n",
    "        x = self.up3(x, x2)\n",
    "        x = self.up4(x, x1)\n",
    "        x = self.outc(x)\n",
    "        return F.sigmoid(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\torch\\nn\\functional.py:1350: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 64, 224, 224]           1,792\n",
      "       BatchNorm2d-2         [-1, 64, 224, 224]             128\n",
      "              ReLU-3         [-1, 64, 224, 224]               0\n",
      "            Conv2d-4         [-1, 64, 224, 224]          36,928\n",
      "       BatchNorm2d-5         [-1, 64, 224, 224]             128\n",
      "              ReLU-6         [-1, 64, 224, 224]               0\n",
      "       double_conv-7         [-1, 64, 224, 224]               0\n",
      "            inconv-8         [-1, 64, 224, 224]               0\n",
      "         MaxPool2d-9         [-1, 64, 112, 112]               0\n",
      "           Conv2d-10        [-1, 128, 112, 112]          73,856\n",
      "      BatchNorm2d-11        [-1, 128, 112, 112]             256\n",
      "             ReLU-12        [-1, 128, 112, 112]               0\n",
      "           Conv2d-13        [-1, 128, 112, 112]         147,584\n",
      "      BatchNorm2d-14        [-1, 128, 112, 112]             256\n",
      "             ReLU-15        [-1, 128, 112, 112]               0\n",
      "      double_conv-16        [-1, 128, 112, 112]               0\n",
      "             down-17        [-1, 128, 112, 112]               0\n",
      "        MaxPool2d-18          [-1, 128, 56, 56]               0\n",
      "           Conv2d-19          [-1, 256, 56, 56]         295,168\n",
      "      BatchNorm2d-20          [-1, 256, 56, 56]             512\n",
      "             ReLU-21          [-1, 256, 56, 56]               0\n",
      "           Conv2d-22          [-1, 256, 56, 56]         590,080\n",
      "      BatchNorm2d-23          [-1, 256, 56, 56]             512\n",
      "             ReLU-24          [-1, 256, 56, 56]               0\n",
      "      double_conv-25          [-1, 256, 56, 56]               0\n",
      "             down-26          [-1, 256, 56, 56]               0\n",
      "        MaxPool2d-27          [-1, 256, 28, 28]               0\n",
      "           Conv2d-28          [-1, 512, 28, 28]       1,180,160\n",
      "      BatchNorm2d-29          [-1, 512, 28, 28]           1,024\n",
      "             ReLU-30          [-1, 512, 28, 28]               0\n",
      "           Conv2d-31          [-1, 512, 28, 28]       2,359,808\n",
      "      BatchNorm2d-32          [-1, 512, 28, 28]           1,024\n",
      "             ReLU-33          [-1, 512, 28, 28]               0\n",
      "      double_conv-34          [-1, 512, 28, 28]               0\n",
      "             down-35          [-1, 512, 28, 28]               0\n",
      "        MaxPool2d-36          [-1, 512, 14, 14]               0\n",
      "           Conv2d-37          [-1, 512, 14, 14]       2,359,808\n",
      "      BatchNorm2d-38          [-1, 512, 14, 14]           1,024\n",
      "             ReLU-39          [-1, 512, 14, 14]               0\n",
      "           Conv2d-40          [-1, 512, 14, 14]       2,359,808\n",
      "      BatchNorm2d-41          [-1, 512, 14, 14]           1,024\n",
      "             ReLU-42          [-1, 512, 14, 14]               0\n",
      "      double_conv-43          [-1, 512, 14, 14]               0\n",
      "             down-44          [-1, 512, 14, 14]               0\n",
      "         Upsample-45          [-1, 512, 28, 28]               0\n",
      "           Conv2d-46          [-1, 256, 28, 28]       2,359,552\n",
      "      BatchNorm2d-47          [-1, 256, 28, 28]             512\n",
      "             ReLU-48          [-1, 256, 28, 28]               0\n",
      "           Conv2d-49          [-1, 256, 28, 28]         590,080\n",
      "      BatchNorm2d-50          [-1, 256, 28, 28]             512\n",
      "             ReLU-51          [-1, 256, 28, 28]               0\n",
      "      double_conv-52          [-1, 256, 28, 28]               0\n",
      "               up-53          [-1, 256, 28, 28]               0\n",
      "         Upsample-54          [-1, 256, 56, 56]               0\n",
      "           Conv2d-55          [-1, 128, 56, 56]         589,952\n",
      "      BatchNorm2d-56          [-1, 128, 56, 56]             256\n",
      "             ReLU-57          [-1, 128, 56, 56]               0\n",
      "           Conv2d-58          [-1, 128, 56, 56]         147,584\n",
      "      BatchNorm2d-59          [-1, 128, 56, 56]             256\n",
      "             ReLU-60          [-1, 128, 56, 56]               0\n",
      "      double_conv-61          [-1, 128, 56, 56]               0\n",
      "               up-62          [-1, 128, 56, 56]               0\n",
      "         Upsample-63        [-1, 128, 112, 112]               0\n",
      "           Conv2d-64         [-1, 64, 112, 112]         147,520\n",
      "      BatchNorm2d-65         [-1, 64, 112, 112]             128\n",
      "             ReLU-66         [-1, 64, 112, 112]               0\n",
      "           Conv2d-67         [-1, 64, 112, 112]          36,928\n",
      "      BatchNorm2d-68         [-1, 64, 112, 112]             128\n",
      "             ReLU-69         [-1, 64, 112, 112]               0\n",
      "      double_conv-70         [-1, 64, 112, 112]               0\n",
      "               up-71         [-1, 64, 112, 112]               0\n",
      "         Upsample-72         [-1, 64, 224, 224]               0\n",
      "           Conv2d-73         [-1, 64, 224, 224]          73,792\n",
      "      BatchNorm2d-74         [-1, 64, 224, 224]             128\n",
      "             ReLU-75         [-1, 64, 224, 224]               0\n",
      "           Conv2d-76         [-1, 64, 224, 224]          36,928\n",
      "      BatchNorm2d-77         [-1, 64, 224, 224]             128\n",
      "             ReLU-78         [-1, 64, 224, 224]               0\n",
      "      double_conv-79         [-1, 64, 224, 224]               0\n",
      "               up-80         [-1, 64, 224, 224]               0\n",
      "           Conv2d-81          [-1, 3, 224, 224]             195\n",
      "          outconv-82          [-1, 3, 224, 224]               0\n",
      "================================================================\n",
      "Total params: 13,395,459\n",
      "Trainable params: 13,395,459\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 715.09\n",
      "Params size (MB): 51.10\n",
      "Estimated Total Size (MB): 766.77\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = UNet(3,3).to(device)\n",
    "\n",
    "summary(model, (3, 224, 224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
